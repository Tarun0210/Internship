{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c64eba",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb24290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Downloading imdb top 100 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movies = soup.select('td.titleColumn')\n",
    "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
    "ratings = [b.attrs.get('data-value')\n",
    "\t\tfor b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a empty list for storing\n",
    "# movie information\n",
    "list = []\n",
    "\n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "\t\n",
    "\t# Separating movie into: 'place',\n",
    "\t# 'title', 'year'\n",
    "\tmovie_string = movies[index].get_text()\n",
    "\tmovie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "\tmovie_title = movie[len(str(index))+1:-7]\n",
    "\tyear = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "\tplace = movie[:len(str(index))-(len(movie))]\n",
    "\tdata = {\"place\": place,\n",
    "\t\t\t\"movie_title\": movie_title,\n",
    "\t\t\t\"rating\": ratings[index],\n",
    "\t\t\t\"year\": year,\n",
    "\t\t\t\"star_cast\": crew[index],\n",
    "\t\t\t}\n",
    "\tlist.append(data)\n",
    "\n",
    "# printing movie details with its rating.\n",
    "for movie in list:\n",
    "\tprint(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "\t\t') -', 'Starring:', movie['star_cast'], movie['rating'])\n",
    "\n",
    "\n",
    "##.......##\n",
    "df = pd.DataFrame(list)\n",
    "df.to_csv('imdb_top_100_movies.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879fb30",
   "metadata": {},
   "source": [
    "# Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54b34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "posts = soup.find_all('div', class_='post')\n",
    "\n",
    "for post in posts:\n",
    "  heading = post.find('h3').text\n",
    "  date = post.find('time')['datetime']\n",
    "  content = post.find('div', class_='post__content').text\n",
    "  youtube_link = post.find('a', class_='post__youtube-link')['href']\n",
    "  likes = post.find('span', class_='post__likes').text\n",
    "\n",
    "  print(f'Heading: {heading}')\n",
    "  print(f'Date: {date}')\n",
    "  print(f'Content: {content}')\n",
    "  print(f'YouTube Link: {youtube_link}')\n",
    "  print(f'Likes: {likes}')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb9847",
   "metadata": {},
   "source": [
    "# Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fe6686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for: Indira Nagar\n",
      "Fetching data for: Jayanagar\n",
      "Fetching data for: Rajaji Nagar\n",
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, EMI, Price, Locality]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get house details from a given URL\n",
    "def get_house_details(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Lists to hold house details\n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "    \n",
    "    # Find all house listings on the page\n",
    "    listings = soup.find_all('div', class_='nb__1pO3e')\n",
    "\n",
    "    for listing in listings:\n",
    "        # Extract house title\n",
    "        title_tag = listing.find('h2', class_='nb__1wYw4')\n",
    "        title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "        \n",
    "        # Extract house location\n",
    "        location_tag = listing.find('div', class_='nb__1CSfO')\n",
    "        location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "        \n",
    "        # Extract area\n",
    "        area_tag = listing.find('div', class_='nb__2eG9F')\n",
    "        area = area_tag.text.strip() if area_tag else 'N/A'\n",
    "        \n",
    "        # Extract EMI (if available)\n",
    "        emi_tag = listing.find('div', class_='nb__2_OuS')\n",
    "        emi = emi_tag.text.strip() if emi_tag else 'N/A'\n",
    "        \n",
    "        # Extract price\n",
    "        price_tag = listing.find('div', class_='nb__2i5Zt')\n",
    "        price = price_tag.text.strip() if price_tag else 'N/A'\n",
    "        \n",
    "        # Append details to lists\n",
    "        titles.append(title)\n",
    "        locations.append(location)\n",
    "        areas.append(area)\n",
    "        emis.append(emi)\n",
    "        prices.append(price)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Title': titles,\n",
    "        'Location': locations,\n",
    "        'Area': areas,\n",
    "        'EMI': emis,\n",
    "        'Price': prices\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Localities to search for\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "base_url = 'https://www.nobroker.in/'\n",
    "\n",
    "# DataFrame to collect data from all localities\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Fetch details for each locality\n",
    "for locality in localities:\n",
    "    search_url = f\"{base_url}/search?query={locality.replace(' ', '%20')}\"\n",
    "    print(f\"Fetching data for: {locality}\")\n",
    "    locality_df = get_house_details(search_url)\n",
    "    locality_df['Locality'] = locality\n",
    "    all_data = pd.concat([all_data, locality_df], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(all_data)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "all_data.to_csv('house_details_nobroker.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c37cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Bewakoof Bestsellers page\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "# Function to scrape product details from the Bewakoof page\n",
    "def get_product_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Lists to hold product details\n",
    "    product_names = []\n",
    "    prices = []\n",
    "    image_urls = []\n",
    "    \n",
    "    # Find all product containers\n",
    "    product_containers = soup.find_all('div', class_='product')\n",
    "    \n",
    "    # Scrape details for the first 10 products\n",
    "    for container in product_containers[:10]:\n",
    "        # Extract product name\n",
    "        name_tag = container.find('h2', class_='product-name')\n",
    "        product_name = name_tag.text.strip() if name_tag else 'N/A'\n",
    "        \n",
    "        # Extract price\n",
    "        price_tag = container.find('span', class_='product-price')\n",
    "        price = price_tag.text.strip() if price_tag else 'N/A'\n",
    "        \n",
    "        # Extract image URL\n",
    "        image_tag = container.find('img', class_='product-image')\n",
    "        image_url = image_tag['src'] if image_tag else 'N/A'\n",
    "        \n",
    "        # Append details to lists\n",
    "        product_names.append(product_name)\n",
    "        prices.append(price)\n",
    "        image_urls.append(image_url)\n",
    "    \n",
    "    # Return the scraped data\n",
    "    return {\n",
    "        'Product Name': product_names,\n",
    "        'Price': prices,\n",
    "        'Image URL': image_urls\n",
    "    }\n",
    "\n",
    "# Get product details\n",
    "product_details = get_product_details(url)\n",
    "\n",
    "# Print product details\n",
    "for i in range(len(product_details['Product Name'])):\n",
    "    print(f\"Product Name: {product_details['Product Name'][i]}\")\n",
    "    print(f\"Price: {product_details['Price'][i]}\")\n",
    "    print(f\"Image URL: {product_details['Image URL'][i]}\")\n",
    "    print('---')\n",
    "\n",
    "# Optionally, you can save the data to a CSV file using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(product_details)\n",
    "df.to_csv('bewakoof_bestsellers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b098bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the CNBC World news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Function to get news details from the CNBC page\n",
    "def get_news_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Lists to hold news details\n",
    "    headings = []\n",
    "    dates = []\n",
    "    links = []\n",
    "    \n",
    "    # Find all news articles\n",
    "    articles = soup.find_all('article', class_='LatestNews-article')\n",
    "    \n",
    "    # Extract details from each article\n",
    "    for article in articles:\n",
    "        # Extract the headline\n",
    "        heading_tag = article.find('a', class_='LatestNews-headline')\n",
    "        heading = heading_tag.text.strip() if heading_tag else 'N/A'\n",
    "        \n",
    "        # Extract the date (if available)\n",
    "        date_tag = article.find('time')\n",
    "        date = date_tag.text.strip() if date_tag else 'N/A'\n",
    "        \n",
    "        # Extract the news link\n",
    "        link_tag = article.find('a', class_='LatestNews-headline')\n",
    "        link = link_tag['href'] if link_tag and link_tag['href'].startswith('https') else 'N/A'\n",
    "        \n",
    "        # Append details to lists\n",
    "        headings.append(heading)\n",
    "        dates.append(date)\n",
    "        links.append(link)\n",
    "    \n",
    "    # Return the scraped data\n",
    "    return {\n",
    "        'Heading': headings,\n",
    "        'Date': dates,\n",
    "        'Link': links\n",
    "    }\n",
    "\n",
    "# Get news details\n",
    "news_details = get_news_details(url)\n",
    "\n",
    "# Print news details\n",
    "for i in range(len(news_details['Heading'])):\n",
    "    print(f\"Heading: {news_details['Heading'][i]}\")\n",
    "    print(f\"Date: {news_details['Date'][i]}\")\n",
    "    print(f\"Link: {news_details['Link'][i]}\")\n",
    "    print('---')\n",
    "\n",
    "# Optionally, you can save the data to a CSV file using pandas\n",
    "df = pd.DataFrame(news_details)\n",
    "df.to_csv('cnbc_world_news.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecfbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the KeAi Publishing most downloaded articles page\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "# Function to get article details from the KeAi Publishing page\n",
    "def get_article_details(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Lists to hold article details\n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "    \n",
    "    # Find all article containers\n",
    "    article_containers = soup.find_all('div', class_='article-card')\n",
    "    \n",
    "    # Extract details from each article\n",
    "    for container in article_containers:\n",
    "        # Extract the title\n",
    "        title_tag = container.find('a', class_='title')\n",
    "        title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "        \n",
    "        # Extract the publication date\n",
    "        date_tag = container.find('div', class_='meta-info')\n",
    "        date = date_tag.text.strip().split('|')[0].strip() if date_tag else 'N/A'\n",
    "        \n",
    "        # Extract the authors\n",
    "        author_tags = container.find('div', class_='authors')\n",
    "        authors_list = [author.text.strip() for author in author_tags.find_all('a')] if author_tags else []\n",
    "        author_names = ', '.join(authors_list)\n",
    "        \n",
    "        # Append details to lists\n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        authors.append(author_names)\n",
    "    \n",
    "    # Return the scraped data\n",
    "    return {\n",
    "        'Title': titles,\n",
    "        'Date': dates,\n",
    "        'Authors': authors\n",
    "    }\n",
    "\n",
    "# Get article details\n",
    "article_details = get_article_details(url)\n",
    "\n",
    "# Print article details\n",
    "for i in range(len(article_details['Title'])):\n",
    "    print(f\"Title: {article_details['Title'][i]}\")\n",
    "    print(f\"Date: {article_details['Date'][i]}\")\n",
    "    print(f\"Authors: {article_details['Authors'][i]}\")\n",
    "    print('---')\n",
    "\n",
    "# Optionally, save the data to a CSV file using pandas\n",
    "df = pd.DataFrame(article_details)\n",
    "df.to_csv('keaai_publishing_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b65ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

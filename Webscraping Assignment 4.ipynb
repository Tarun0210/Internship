{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39398af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\tarun\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\tarun\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f063afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import debugger\n",
    "import re\n",
    "\n",
    "# selenium\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "# Beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# add time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed73ec",
   "metadata": {},
   "source": [
    "# Q1 : Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    " Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    " You need to find following details:\n",
    " A) Rank\n",
    " B) Name\n",
    " C) Artist\n",
    " D) Upload date\n",
    " E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d897acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, connect to the webdriver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7b642528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for scraping the data\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37df650b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[A]</td>\n",
       "      <td>Baby Shark D</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>15.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A]</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>15.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank              Name                                       Artist  \\\n",
       "0  [A]      Baby Shark D  Pinkfong Baby Shark - Kids' Songs & Stories   \n",
       "1  [A]  Baby Shark Dance  Pinkfong Baby Shark - Kids' Songs & Stories   \n",
       "\n",
       "     Upload Date Views (in Billions)  \n",
       "0  June 17, 2016               15.12  \n",
       "1  June 17, 2016               15.12  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping Rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[1]/tbody/tr[1]/td[5]/sup/a'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "        \n",
    "# Scraping Name of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[1]/tbody/tr[1]/td[1]'):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"-\")\n",
    "        \n",
    "# Scraping Artist of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[1]/tbody/tr[1]/td[2]'):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\")\n",
    "        \n",
    "# Scraping Upload_Date of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[1]/tbody/tr[1]/td[4]'):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append(\"-\")\n",
    "        \n",
    "# Scraping Views of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[1]/tbody/tr[1]/td[3]'):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "        \n",
    "# creating DataFrame for scraped data\n",
    "Wiki = pd.DataFrame({})\n",
    "Wiki['Rank'] = Rank\n",
    "Wiki['Name'] = Name\n",
    "Wiki['Artist'] = Artist\n",
    "Wiki['Upload Date'] = Date\n",
    "Wiki['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Wiki.Name = Wiki.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050b54c",
   "metadata": {},
   "source": [
    "# Q2 : Scrape the details team India’s international fixtures from bcci.tv.\n",
    " Url = https://www.bcci.tv/.\n",
    " You need to find following details:\n",
    "  A) Match title (I.e. 1st ODI)\n",
    "  B) Series\n",
    "  C) Place\n",
    "  D) Date\n",
    "  E) Time\n",
    " \n",
    "    Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "539fb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0a710f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn=driver.find_element(By.XPATH,'/html/body/header/div[3]/div[2]/ul/div[1]/a[2]')\n",
    "driver.get(btn.get_attribute(\"href\"))\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping the data\n",
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "75846202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India\\nBangladesh</td>\n",
       "      <td>Bangladesh Tour of India T20 Series 2024</td>\n",
       "      <td>Arun Jaitley Stadium, Delhi</td>\n",
       "      <td>9 Oct</td>\n",
       "      <td>Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India\\nBangladesh</td>\n",
       "      <td>Bangladesh Tour of India T20 Series 2024</td>\n",
       "      <td>Rajiv Gandhi International Stadium, Hyderabad</td>\n",
       "      <td>12 Oct</td>\n",
       "      <td>Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India\\nNew Zealand</td>\n",
       "      <td>New Zealand Tour of India Test Series 2024</td>\n",
       "      <td>M Chinnaswamy Stadium, Bengaluru</td>\n",
       "      <td>16 Oct</td>\n",
       "      <td>Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India\\nNew Zealand</td>\n",
       "      <td>New Zealand Tour of India Test Series 2024</td>\n",
       "      <td>Maharashtra Cricket Association Stadium, Pune</td>\n",
       "      <td>24 Oct</td>\n",
       "      <td>Oct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India\\nNew Zealand</td>\n",
       "      <td>New Zealand Tour of India Test Series 2024</td>\n",
       "      <td>Wankhede Stadium, Mumbai</td>\n",
       "      <td>1 Nov</td>\n",
       "      <td>Nov</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Match Title                                      Series  \\\n",
       "0   India\\nBangladesh    Bangladesh Tour of India T20 Series 2024   \n",
       "1   India\\nBangladesh    Bangladesh Tour of India T20 Series 2024   \n",
       "2  India\\nNew Zealand  New Zealand Tour of India Test Series 2024   \n",
       "3  India\\nNew Zealand  New Zealand Tour of India Test Series 2024   \n",
       "4  India\\nNew Zealand  New Zealand Tour of India Test Series 2024   \n",
       "\n",
       "                                           Place    Date Time  \n",
       "0                    Arun Jaitley Stadium, Delhi   9 Oct  Oct  \n",
       "1  Rajiv Gandhi International Stadium, Hyderabad  12 Oct  Oct  \n",
       "2               M Chinnaswamy Stadium, Bengaluru  16 Oct  Oct  \n",
       "3  Maharashtra Cricket Association Stadium, Pune  24 Oct  Oct  \n",
       "4                       Wankhede Stadium, Mumbai   1 Nov  Nov  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"match-card\"]/div[2]/div/div'):\n",
    "    Match_Title.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"match-card\"]/div[1]/div/div[2]/h5'):\n",
    "    Series.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"match-card\"]/div[1]/div/div[2]/div[3]'):\n",
    "    Place.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"match-card\"]/div[1]/div/div[2]/div[1]/div[1]'):\n",
    "    Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})\n",
    "fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab32260",
   "metadata": {},
   "source": [
    "# Q3 : Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "  Url = http://statisticstimes.com/\n",
    "  You have to find following details:\n",
    "  A) Rank\n",
    "  B) State\n",
    "  C) GSDP at current price (19-20)\n",
    "  D) GSDP at current price (18-19)\n",
    "  E) Share(18-19)\n",
    "  F) GDP($ billion)\n",
    "    Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8acc7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://statisticstimes.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2b85fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Economy button\n",
    "driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button').click()\n",
    "\n",
    "# clicking on India\n",
    "driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[1]/div').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[1]/ul/li[2]/a').click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "32a68d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share (18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>4,044,251</td>\n",
       "      <td>3,645,884</td>\n",
       "      <td>13.53%</td>\n",
       "      <td>453.674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank        State GSDP at current price (19-20)  \\\n",
       "0    1  Maharashtra                     4,044,251   \n",
       "\n",
       "  GSDP at current price (18-19) Share (18-19) GDP($ billion)  \n",
       "0                     3,645,884        13.53%        453.674  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []\n",
    "\n",
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[1]'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[2]'):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (23-24)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[3]'):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (22-23)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[4]'):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (22-23)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[5]'):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[6]'):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")\n",
    "    \n",
    "    \n",
    "# creating DataFrame from the scraped data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share (18-19)'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77aea2c",
   "metadata": {},
   "source": [
    "# Q4 :Scrape the details of trending repositories on Github.com.\n",
    "  Url = https://github.com/\n",
    "  You have to find the following details:\n",
    "  A) Repository title\n",
    "  B) Repository description\n",
    "  C) Contributors count\n",
    "  D) Language used\n",
    "    Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "165cd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://github.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c5ba6cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Repository Title, Repository Description, Contributors Count, Language Used]\n",
       "Index: []"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "# fetching urls for each repository\n",
    "repository = driver.find_elements(By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[1]/div/div/div[1]/span/a')\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements(By.XPATH,'/html/body/div[1]/div[1]/header/div[1]/div[1]/div/div[2]/nav/ul/li[1]/a/span')\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n",
    "    \n",
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element(By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[1]/div/div/p')\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "    # scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element(By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[6]/div/div/a')\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "    \n",
    "    \n",
    "    # scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements(By.XPATH,'//*[@id=\"repo-content-pjax-container\"]/div/div/div[2]/div[2]/div/div[7]/div/ul/li[1]/a/span[1]'):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')\n",
    "        \n",
    "        \n",
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Contributors Count'] = Contributors\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed87b1e",
   "metadata": {},
   "source": [
    "# Q5 Scrape the details of top 100 songs on billboard.com.\n",
    "     Url = https://www.billboard.com/\n",
    "     You have to find the following details:\n",
    "     A) Song name\n",
    "     B) Artist name\n",
    "     C) Last week rank\n",
    "     D) Peak rank\n",
    "     E) Weeks on board\n",
    "      Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "51fb7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.billboard.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2f237f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Bar Song (Tipsy)</td>\n",
       "      <td>Shaboozey</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>WKS ON CHART</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name     Artist Last Week Rank Peak Rank Weeks on board\n",
       "0  A Bar Song (Tipsy)  Shaboozey              1         1   WKS ON CHART"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name =[]\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []\n",
    "\n",
    "\n",
    "\n",
    "# scraping data of song names\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[2]/div/div[1]/div[1]/div[1]/div/div[2]/h3/a'):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[2]/div/div[1]/div[1]/div[1]/div/div[2]/p'):\n",
    "    Artist_Name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[4]'):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[5]'):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"post-1479786\"]/div[3]/div/div/div/div[2]/div[1]/div[9]'):\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "billiboard = pd.DataFrame({})\n",
    "billiboard['Name'] = Song_Name\n",
    "billiboard['Artist'] = Artist_Name\n",
    "billiboard['Last Week Rank'] = Last_week_rank\n",
    "billiboard['Peak Rank'] = Peak_rank\n",
    "billiboard['Weeks on board'] = Weeks_on_board\n",
    "billiboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2285b4",
   "metadata": {},
   "source": [
    "# Q6 Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5558f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4f946d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Book Name      Author Volume sold   Publisher  \\\n",
       "0  Da Vinci Code,The  Brown, Dan   5,094,805  Transworld   \n",
       "\n",
       "                         Genre  \n",
       "0  Crime, Thriller & Adventure  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "\n",
    "\n",
    "# scraping book names data\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[1]/td[2]'):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[1]/td[3]'):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[1]/td[4]'):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[1]/td[5]'):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "# scraping  data of genre\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[1]/td[6]'):\n",
    "    Genre.append(i.text)    \n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372b73e",
   "metadata": {},
   "source": [
    "# Q7 Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "81fb9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.imdb.com/list/ls095964455/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fa540281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joker: Folie à Deux</td>\n",
       "      <td>October 4, 2024 (United States)</td>\n",
       "      <td>CrimeDramaMusicalThriller</td>\n",
       "      <td>2h 18m</td>\n",
       "      <td>5.3</td>\n",
       "      <td>46K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name                        Year Span  \\\n",
       "0  Joker: Folie à Deux  October 4, 2024 (United States)   \n",
       "\n",
       "                       Genre Run Time Ratings Votes  \n",
       "0  CrimeDramaMusicalThriller   2h 18m     5.3   46K  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n",
    "\n",
    "# scraped data of Names\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[2]/div[1]/h1/span'):\n",
    "    Name.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Year span\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[13]/div[2]/ul/li[1]/div/ul/li/a'):\n",
    "    Year_span.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Genre\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[7]/div[2]/ul[2]/li[2]/div'):\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Run time\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[2]/div[1]/ul/li[3]'):\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraped data of Ratings\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[2]/div[1]/div/div[1]/a/span/div/div[2]/div[1]/span[1]'):\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "# scraped data of Votes\n",
    "for i in driver.find_elements(By.XPATH,'//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[2]/div[1]/div/div[1]/a/span/div/div[2]/div[3]'):\n",
    "    Votes.append(i.text) \n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "TV_Series = pd.DataFrame({})\n",
    "TV_Series['Name'] = Name\n",
    "TV_Series['Year Span'] = Year_span\n",
    "TV_Series['Genre'] = Genre\n",
    "TV_Series['Run Time'] = Run_time\n",
    "TV_Series['Ratings'] = Ratings\n",
    "TV_Series['Votes'] = Votes\n",
    "TV_Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f1548",
   "metadata": {},
   "source": [
    "# Q8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "57a2b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\" https://archive.ics.uci.edu/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e7ad7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1204ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/h2/a')\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f9060093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[1]/div/div[2]/div/h1')\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "      # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[1]/p')\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[3]/p')\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[4]/p')\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[5]/p')\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Attributes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[6]/p')\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # scraping Year\n",
    "    try:\n",
    "        year = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[1]/div/div[2]/h2')\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "755ce3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>Dona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Name Data Type            Task  Attribute Type  No of Instance   \\\n",
       "0         Iris    Tabular  Classification            Real             150   \n",
       "\n",
       "  No of Attributes  Year   \n",
       "0                 4  Dona  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "ML = pd.DataFrame({})\n",
    "ML['Dataset Name'] = Dataset_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a8d2711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb9a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
